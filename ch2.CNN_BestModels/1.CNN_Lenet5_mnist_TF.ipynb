{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_layers_sparse(train_set, test_set):\n",
    "    x_train, y_train = train_set.images, train_set.labels\n",
    "    x_test, y_test = test_set.images, test_set.labels\n",
    "\n",
    "    #------------------------------------------------------\n",
    "\n",
    "    # [5, 5, 1, 6] : filter(5,5), ch(1), filter_no(6)\n",
    "    w1 = tf.get_variable('w1', shape=[5, 5, 1, 6],\n",
    "                         initializer=tf.glorot_uniform_initializer)  # xavier initialization\n",
    "    w2 = tf.get_variable('w2', shape=[5, 5, 6, 16],\n",
    "                         initializer=tf.glorot_uniform_initializer)\n",
    "    w3 = tf.get_variable('w3', shape=[400, 120],\n",
    "                         initializer=tf.glorot_uniform_initializer)\n",
    "    w4 = tf.get_variable('w4', shape=[120, 84],\n",
    "                         initializer=tf.glorot_uniform_initializer)\n",
    "    w5 = tf.get_variable('w5', shape=[84, 10],\n",
    "                         initializer=tf.glorot_uniform_initializer)\n",
    "    b1 = tf.Variable(tf.zeros([6]))\n",
    "    b2 = tf.Variable(tf.zeros([16]))\n",
    "    b3 = tf.Variable(tf.zeros([120]))\n",
    "    b4 = tf.Variable(tf.zeros([84]))\n",
    "    b5 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    ph_x = tf.placeholder(tf.float32)\n",
    "    ph_y = tf.placeholder(tf.int32)\n",
    "\n",
    "    #------------------------------------------------------\n",
    "\n",
    "    # out:(?, 28, 28, 6), same or valid\n",
    "    # stride크기 [1,1,1,1]:NHWC(batch_size, Height, Width, Channel)\n",
    "    c1 = tf.nn.conv2d(ph_x, w1, [1, 1, 1, 1], 'SAME')\n",
    "    r1 = tf.nn.relu(c1 + b1)\n",
    "    # [1, 2, 2, 1], [1, 2, 2, 1] : filter & stride\n",
    "    # out:(?, 14, 14, 6)\n",
    "    p1 = tf.nn.max_pool2d(r1, [1, 2, 2, 1], [1, 2, 2, 1], 'VALID')\n",
    "    print(c1.shape)  # (?, ?, ?, 6)\n",
    "    print(r1.shape)  # (?, ?, ?, 6)\n",
    "    print(p1.shape)  # (?, ?, ?, 6)\n",
    "\n",
    "    # out:(?, 10, 10, 16), same or valid\n",
    "    c2 = tf.nn.conv2d(p1, w2, [1, 1, 1, 1], 'VALID')\n",
    "    # c2 = tf.nn.conv2d(p1, w2, [1, 1, 1, 1])\n",
    "    r2 = tf.nn.relu(c2 + b2)\n",
    "    # out:(?, 5, 5, 16)\n",
    "    p2 = tf.nn.max_pool2d(r2, [1, 2, 2, 1], [1, 2, 2, 1], 'VALID')\n",
    "    # p2 = tf.nn.max_pool2d(r2, [1, 2, 2, 1], [1, 2, 2, 1])\n",
    "    print(c2.shape)  # (?, ?, ?, 6)\n",
    "    print(r2.shape)  # (?, ?, ?, 16)\n",
    "    print(p2.shape)  # (?, ?, ?, 16)\n",
    "\n",
    "    flat = tf.reshape(p2, (-1, 5 * 5 * 16))\n",
    "    print(flat.shape)  # (?, 400)\n",
    "    # exit(-1)\n",
    "\n",
    "\n",
    "    z3 = tf.matmul(flat, w3) + b3\n",
    "    r3 = tf.nn.sigmoid(z3)\n",
    "    z4 = tf.matmul(r3, w4) + b4\n",
    "    r4 = tf.nn.sigmoid(z4)\n",
    "    z = tf.matmul(r4, w5) + b5\n",
    "\n",
    "    loss_i = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=z,\n",
    "                                                        labels=ph_y)\n",
    "    loss = tf.reduce_mean(loss_i)\n",
    "\n",
    "    # optimizer = tf.train.GradientDescentOptimizer(0.1)  #\n",
    "    # optimizer = tf.train.GradientDescentOptimizer(0.01)  #\n",
    "    # optimizer = tf.train.AdamOptimizer(0.01)  #\n",
    "    optimizer = tf.train.RMSPropOptimizer(0.01)  #\n",
    "    train = optimizer.minimize(loss)\n",
    "    #------------------------------------------------------\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    epochs = 10\n",
    "    batch_size = 128\n",
    "    n_iteration = len(x_train )// batch_size\n",
    "\n",
    "    for i in range(epochs):\n",
    "        total = 0\n",
    "        for j in range(n_iteration):\n",
    "            # n1 = j * batch_size\n",
    "            # n2 = n1 + batch_size\n",
    "            #\n",
    "            # xx = x_train[n1:n2]\n",
    "            # yy = y_train[n1:n2]\n",
    "\n",
    "            xx, yy = train_set.next_batch(batch_size)  # epoch 마다 shuffle하는 기능이 내장됨!!\n",
    "\n",
    "            xx = xx.reshape(-1, 28, 28, 1)\n",
    "\n",
    "            sess.run(train, {ph_x: xx, ph_y: yy})\n",
    "            total += sess.run(loss, {ph_x: xx, ph_y: yy})\n",
    "            # break\n",
    "\n",
    "        # break\n",
    "        print(i, total / n_iteration)\n",
    "    print('-' * 50)\n",
    "\n",
    "    xx = x_test.reshape(-1, 28, 28, 1)\n",
    "    preds = sess.run(z, {ph_x: xx})\n",
    "    # print(preds)\n",
    "    pred_arg = np.argmax(preds, axis=1)\n",
    "    # print(pred_arg)\n",
    "\n",
    "    print('acc:', np.mean(pred_arg == y_test))\n",
    "\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-2257e450f145>:6: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\shkim\\Anaconda3\\envs\\tutorial\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\shkim\\Anaconda3\\envs\\tutorial\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting mnist\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\shkim\\Anaconda3\\envs\\tutorial\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\shkim\\Anaconda3\\envs\\tutorial\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "(?, ?, ?, 6)\n",
      "(?, ?, ?, 6)\n",
      "(?, ?, ?, 6)\n",
      "(?, ?, ?, 16)\n",
      "(?, ?, ?, 16)\n",
      "(?, ?, ?, 16)\n",
      "(?, 400)\n",
      "WARNING:tensorflow:From C:\\Users\\shkim\\Anaconda3\\envs\\tutorial\\lib\\site-packages\\tensorflow_core\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "0 0.6404683382840858\n",
      "1 0.016375990094137236\n",
      "2 0.010521256128116566\n",
      "3 0.008682951154955282\n",
      "4 0.008529312769507356\n",
      "5 0.009112062745826375\n",
      "6 0.009192535741700267\n",
      "7 0.012072889566706983\n",
      "8 0.011288260173221881\n",
      "9 0.011201075732622226\n",
      "--------------------------------------------------\n",
      "acc: 0.9824\n"
     ]
    }
   ],
   "source": [
    "# print(mnist.train.images.shape)       # (55000, 784)   784:28*28\n",
    "# print(mnist.validation.images.shape)  # (5000, 784)\n",
    "# print(mnist.test.images.shape)        # (10000, 784)\n",
    "# print(mnist.train.labels.shape)       # (55000, 10)\n",
    "\n",
    "mnist = input_data.read_data_sets('mnist') # 60k train-sets, 10k test-sets\n",
    "multi_layers_sparse(mnist.train, mnist.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
